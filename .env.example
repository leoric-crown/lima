# LIMA - Local Intelligent Memo Assistant
# Environment Configuration
#
# Copy this file to .env and update the values
# GENERATE comments indicate auto-generation directives for setup-env.py

# =============================================================================
# Network Binding
# =============================================================================

# Host to bind services to:
#   127.0.0.1 = localhost only (secure default, works with Tailscale)
#   0.0.0.0   = all interfaces (exposes to LAN - use with caution)
BIND_HOST=127.0.0.1

# =============================================================================
# PostgreSQL Configuration
# =============================================================================

# Root postgres user (used for initialization only)
POSTGRES_USER=postgres
# GENERATE: strong_password(32) | Manual: openssl rand -base64 32
POSTGRES_PASSWORD=change_me_postgres_password
POSTGRES_DB=lima
POSTGRES_PORT=5432

# =============================================================================
# n8n Database User (non-root, principle of least privilege)
# =============================================================================

N8N_DB_USER=n8n_user
# GENERATE: strong_password(32) | Manual: openssl rand -base64 32
N8N_DB_PASSWORD=change_me_n8n_db_password

# =============================================================================
# n8n Application Configuration
# =============================================================================

# Protocol: http for local, https for production
N8N_PROTOCOL=http
# Host: localhost for local, your domain for production
N8N_HOST=localhost
N8N_PORT=5678

# Webhook URL (auto-constructed from protocol/host/port)
# GENERATE: template("n8n_webhook_url") | Manual: Set your full webhook URL
WEBHOOK_URL=http://localhost:5678/

# GENERATE: hex_key(32) | Manual: openssl rand -hex 32
N8N_ENCRYPTION_KEY=change_me_encryption_key

# GENERATE: auto_detect_timezone | Manual: Set your timezone (e.g., America/New_York)
TIMEZONE=America/Mexico_City

# =============================================================================
# n8n-mcp Configuration (Development Only)
# =============================================================================

# Auth token for MCP server
# GENERATE: hex_key(32) | Manual: openssl rand -hex 32
MCP_AUTH_TOKEN=change_me_mcp_auth_token

# n8n API key (generate in n8n UI: Settings > API > Create API Key)
# GENERATE: manual | Manual: Generate in n8n after first setup
N8N_API_KEY=

# MCP server port
MCP_PORT=8042

# Log level: error, warn, info, debug
MCP_LOG_LEVEL=info

# =============================================================================
# Local LLM Configuration
# =============================================================================

# Port for your local LLM server. Used by `make seed` to configure credentials.
# Default: 1234 (LM Studio)
# Ollama users: set to 11434
# LOCAL_LLM_PORT=11434

# LLM model name (must support tool calling). Used by `make seed` to configure workflows.
# Default: openai/gpt-oss-20b (works on macOS, Linux, Windows)
# Format: provider/model-name (as shown in LM Studio or Ollama)
# LLM_MODEL=openai/gpt-oss-20b

# =============================================================================
# Optional: postgres-mcp Configuration
# =============================================================================

POSTGRES_MCP_PORT=8700

# =============================================================================
# Optional: pgAdmin Configuration
# =============================================================================

PGADMIN_EMAIL=admin@example.com
# GENERATE: strong_password(16) | Manual: Choose a password
PGADMIN_PASSWORD=change_me_pgadmin
PGADMIN_PORT=5050

# =============================================================================
# Whisper Configuration (Speech-to-Text)
# =============================================================================

# Model options (smaller = faster, larger = more accurate):
#   Systran/faster-whisper-tiny    (~75MB,  fastest)
#   Systran/faster-whisper-base    (~145MB, good balance) [default]
#   Systran/faster-whisper-small   (~488MB, better accuracy)
#   Systran/faster-whisper-medium  (~1.5GB, high accuracy)
#   Systran/faster-whisper-large-v3 (~3GB,  best accuracy)
WHISPER_MODEL=Systran/faster-whisper-base
WHISPER_PORT=9000

# Native GPU Whisper Server (Optional)
# If using the native CUDA/MLX whisper server instead of Docker Speaches,
# configure the host/port here. Used by `make seed` to configure the workflow.
# See services/whisper-server/README.md for setup instructions.
#
# NATIVE_WHISPER_HOST: Bind address (default: 0.0.0.0, accessible from Docker)
# NATIVE_WHISPER_PORT: Server port (default: 9001)
# NATIVE_WHISPER_HOST=0.0.0.0
# NATIVE_WHISPER_PORT=9001

# =============================================================================
# Caddy Configuration (Voice Recorder Proxy)
# =============================================================================

# Port for the voice recorder UI (served by Caddy)
# Access at: http://localhost:${CADDY_PORT}/lima/recorder/
CADDY_PORT=8888
